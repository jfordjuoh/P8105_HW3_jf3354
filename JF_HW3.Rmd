---
title: "p8105_hw3_jf3354"
author: Judy Fordjuoh
date: October 15, 2021
output: github_document
---

#Loading all the librarys/setting the themes,color schemes, and the graph output sizes 
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(ggridges)
library(patchwork)
library(p8105.datasets)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

**QUESTION 1**
```{r}
data("instacart") #looking at the dataset as a whole. 15 columns and 1,384,617 rows
instacart %>%
  count(aisle) %>%
  arrange(desc(n))

summary(instacart)
```
**Answer: In the instacart data there are 15 columns and 1,384,617 rows. Some variables included the aisle, aisle ID, product id, order numbers, user id, and day of the week which only included 6 days. The data is organized by the order number. There are 134 aisles. The fresh vegetables aisle(n=150609) and fresh fruits aisle (n= 150473) are the aisles where the most items are ordered from, respectively.**

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle Name",
    y = "Number of items"
  ) 
```


```{r}
#Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

#Note to self: %in% checks whether or not the object is contained in the other object.  == is a logical operator that checks for identity properties.

instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(ranking = min_rank(desc(n))) %>%  #ranking the n so that those with the least ranking go to the bottom
  filter(ranking < 4) %>% #so we can get only the the top 3 rankings
  arrange(aisle, ranking) %>%
  knitr::kable()
```
**Answer: In the baking ingredients aisle, Light Brown Sugar (n=499), Pure Baking Soda (n=387), and Cane Sugar (n=336), are the 3 most popular items. In the dog food care aisle, Snack Sticks Chicken & Rice Recipe Dog Treats (n=30), Organix Chicken & Brown Rice Recipe (n=28), and Small Dog Biscuits (n=26), are the 3 most popular items. In the packaged vegetables and fruits aisle, Organic Baby Spinach (n=9784), Organic Raspberries (n=5546), and Organic Blueberries (n=4966), are the 3 most popular items.**

```{r}
#Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(order_dow = recode(order_dow,
                            "0" = "Sunday",
                            "1" = "Monday",
                            "2" = "Tuesday",
                            "3" = "Wedensday",
                            "4" = "Thursday",
                            "5" = "Friday",
                            "6" = "Saturday")) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
 knitr::kable()
```


**QUESTION 2**
```{r} 
#Note to self: You can change the order level of a factor variable to your specified preference using forcats::fct_relevel (visualization pt 2)

#cleaning the data 
brfss <- brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% #include only responses from “Excellent” to “Poor”
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) #organizing the responses as a factor taking levels ordered from “Excellent” to “Poor”
```

```{r}
#Note to self: n_distinct efficiently count the number of unique values in a set of vector. Lecture:exploratory analysis.

#In 2002, which states were observed at 7 or more locations? What about in 2010?
brfss %>% 
  filter(year %in% c("2002", "2010")) %>% 
  group_by(year, locationabbr) %>% 
  summarise(location = n_distinct(locationdesc)) %>% #counting the number of times the location is listed in locationdesc then I'll filtering to only those who were in 7 or more locations
  filter(location >= 7)
```
**Answer: In 2002, CT, FL, and NC, were the three states that were observed at 7 locations. MA and NJ were observed at 8 locations while PA was observed at 10 locations. In 2010, CO, PA and SC were observed at 7 locations, OH was observed at 8 locations, NY and MA were observed at 9 locations, WA and NE were observed at 10 locations, CA, MD, and NC were observed in 12 locations, TX were observed in 16 locations, NJ were observed in 19 locations, and FL were observed in 41 locations.**
  
  
  
```{r}
#Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state 
brfss2 <- brfss %>% 
  rename(state = locationabbr) %>%
  select(year, state, response, data_value) %>%
  filter(response == "Excellent") %>%
  group_by(year,state) %>%
  mutate(average_dv = mean(data_value)) %>%
  ggplot(aes(x = year, y = average_dv, color = state)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Each state average data value, 2002 to 2010",
    x = "Year",
    y = "Average Data Value") +
    theme(legend.position = "right")

brfss2

```

**Answer: Due to me trying to plot many states in one plot, it is quite difficult to tell the difference between each line. However, we can see that there are quite a few states that have an average data value that stays within the range of 20 to 25 throughout the years.**   

```{r}
#Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

brfss3 <- brfss %>% 
  filter(year %in% c("2006","2010") & locationabbr == "NY") %>% 
  ggplot(aes(x = data_value, y = response)) +
  geom_density_ridges(scale = .85) +
  facet_grid(~year) +
  labs(
    title = "Distribution of Data Value by Responses in NYS, 2006 and 2010",
    x = "Data Value",
    y = "Response"
  )

brfss3
```

**Answer:I decided to use a density graph because they are good at showcasing the distribution of a numeric variable in a dataset. In terms of responses, we see that there was not much of a difference in the data values between the poor responses in 2006 and 2010. In the fair responses in 2006, there was a peak in the data value between 10 and 15 (I would say the peak is around 12). However in 2010 we see that the data value range widens a bit and the peak shifts a tad bit (I would say the peak is around 13). In 2006, the good responses had a bimodial distribution with a peak between 25 and 30 and a peak between 35 and 40. In 2010, the distribution of the data value becomes more unimodial with a significant peak at data value = 30. In the 2006 very good responses, we see that the distribution was bimodial with a peak at data value =25 and another peak around data value =34. In 2010, the distribution stays bimodial with a peak at data value =23 and another peak around data value =37 or 38. In the excellent response, we see that it follows a bimodial distribution with a peak at data value =20 and another around data value =27. In 2010 th shsape becomes more unimodal with a significant point at 25.**


**QUESTION 3**
```{r}
#Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

accel_df = 
  read_csv('accel_data.csv') %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity"
  ) %>%
  mutate(day_type = ifelse(day %in% c("Saturday", "Sunday"),"weekend", "weekday"),
         minute = as.integer(minute))
```
**Answer: This dataset contains 5 weeks of accelerometer data collected from a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). There are 50400 rows and 6 columns in the accel_df. The variables in this dataframe are week, day_id, day, minute, activity, and the day_type. Day_type is the variable I created which specifies if a day is a weekday or weekend.**



```{r}
#Using your tidied dataset, aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

accel_df %>%
  mutate(
    day = forcats::fct_relevel(day, c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
  ) %>%
  group_by(week, day) %>%
  summarize(
    total_activity = sum(activity)
  ) %>%
  knitr::kable()
 
```
**Answer: Activity counts were at their lowest on the two Saturdays prior to end of data collection (weeks 4 and 5). This individual is most active on Friday, Saturday, Sunday, and Monday. However the Saturday of Week 4 and Week 5, the individual's activity was not as high as previous saturdays. On each Saturday (week 4 and week 5) the participant had a total activity of 1440 min.**



```{r}
# Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

accel_df %>% 
  ggplot(aes(x = minute, y = activity, color = day)) + 
  geom_line() +
  labs(
    title = "24-Hour Activity Count by Day",
    x = "Time",
    y = "Activity Count"
  ) + 
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440),
    labels = c("12AM", "6AM", "12PM", "6PM", "11:59PM")
    )
```

**Answer: Based on this graph, this indivivdual's is not active during 12am to 6am which is most likely because they are asleep. Generally, this individual seems to be more active around 7/8pm-10/10:30pm.**
